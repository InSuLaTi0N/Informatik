{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_networks_solution.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InSuLaTi0N/Informatik/blob/master/neural_networks_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5WXewmrpZFy"
      },
      "source": [
        "Version: 2022.01.06\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oOq-yv8q3Eu"
      },
      "source": [
        "# Intelligente Systeme - Übung Neuronale Netze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5-6JohCGCSF"
      },
      "source": [
        "## Aufgabe 1 - Von Regression zu Klassifikation\n",
        "\n",
        "Letzte Woche haben wir mittels Gradientenabstieg den funktionalen Zusammenhang zwischen zwei Werten beschrieben. Diese Woche verwenden wir Gradientenabstieg nicht zur Regression, sondern zur Klassifikation.\n",
        "\n",
        "1. Wir klassifizieren mithilfe der logistischen Funktion $\\frac{1}{1+e^{-x}}$, wobei wir für x die nach 0 umgestellte lineare Funktion $y = w_1 x + w_0$ einsetzen, welche die beiden Klassen trennt. Überlegen Sie, welche Bedeutung das Umstellen nach 0 hat. Welche Differenz wird damit berechnet, wenn die Koordinaten eines zu klassifizierenden Punktes eingesetzt werden? Wie wird diese Differenz von der logistischen Funktion interpretiert? \n",
        "\n",
        "  Sie können sich die Zusammenhänge mit Hilfe eines kleinen Beispiels verdeutlichen: Die Funktion $y = w_1x + w_0$ mit $w_1= 1$ und $w_0 = 0.5$ trennt zwei Klassen 0 und 1. Ein Beispiel der Klasse 0 ist der Punkt $P_1=(1,2.5)$, ein Beispiel der Klasse 1 ist $P_2=(2.5,1)$.\n",
        "\n",
        "2. In der Vorlesung haben Sie gelernt, dass die nach 0 umgestellte lineare Funktion auch in der Vektorschreibweise $\\vec{w} \\cdot \\vec{x} = 0$, mit $\\vec{w} = \\begin{pmatrix} w_0 \\\\ w_1 \\\\w_2\\end{pmatrix}, \\qquad \\vec{x} = \\begin{pmatrix}x_0 \\\\ x_1\\\\ x_2\\end{pmatrix}, \\qquad$ dargestellt werden kann. \n",
        "\n",
        "  Wo finden Sie in dieser Darstellung das $x$ und das $y$ aus Aufgabe 1.1? Was sind $w_2$ und $x_0$? \n",
        "\n",
        "\n",
        "3. Der Code unten implementiert den Gradientenabstieg für die lineare Regression für einige Beispieldaten. Die Daten haben jedoch zusätzlich zur x- und y-Koordinate auch ein Klassenlabel. Ergänzen Sie den Code, um Gradientenabstieg zum Lernen eines Klassifikators zu nutzen. Verwenden Sie ihre Erkenntnisse aus Aufgaben 1.1 und 1.2. Warum muss $w_2$ nicht gelernt werden?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJMNWJv287-k"
      },
      "source": [
        "### Lösung - Aufgabe 1.1\n",
        "\n",
        "Die berechnete Differenz entspricht dem Abstand von dem zu klassifizierenden Punkt zu der linearen Funktion, die die beiden Klassen trennt. Mit dem untenstehenden Code können Sie dies für das gegebene Beispiel visualisieren. \n",
        "\n",
        "Dieser Abstand ist \n",
        "* negativ, wenn der Punkt oberhalb der Geraden liegt, \n",
        "* positiv, wenn er darunter liegt und \n",
        "* 0, wenn er genau auf der Geraden liegt. \n",
        "\n",
        "Die Logistische Funktion ordnet dann jedem Punkt mit positiver Differenz einen Wert größer 0.5 (und damit das Label 1) und mit negativer Differenz einen Wert kleiner 0.5 (und damit das Label 0) zu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8O6wrqi86FT"
      },
      "source": [
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  import math\n",
        "\n",
        "  def f(x):\n",
        "    return w1*x+w0\n",
        "  w1 = 1\n",
        "  w0 = 0.5\n",
        "\n",
        "  plt.ylim(0,4)\n",
        "  plt.xlim(0,4)\n",
        "  plt.scatter([1],[2.5])\n",
        "  plt.scatter([2.5],[1])\n",
        "  plt.plot([1,1],[f(1),2.5])\n",
        "  plt.plot([2.5,2.5],[f(2.5),1])\n",
        "  plt.plot([0,4],[f(0),f(4)])\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.show()\n",
        "  \n",
        "  def logistic(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        "\n",
        "  ax = plt.subplot(1, 1, 1)\n",
        "\n",
        "  ax.spines['left'].set_position(('data', 0.0))\n",
        "  ax.spines['bottom'].set_position(('data', 0.0))\n",
        "  ax.spines['right'].set_color('none')\n",
        "  ax.spines['top'].set_color('none')\n",
        "\n",
        "  xs = np.arange(-10,10,0.5)\n",
        "  plt.plot(xs,[logistic(x) for x in xs], label=\"$f(x)=1/(1+e^{-x})$\")\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jbMbLpxEzFX"
      },
      "source": [
        "### Lösung - Aufgabe 1.2\n",
        "$\\vec{w} \\cdot \\vec{x} = w_0x_0 + w_1x_1 + w_2x_2 = 0 = w_0 +w_1x -y$\n",
        "\n",
        "$x$ entspricht also $x_1$ und $y$ entspricht $x_2$.\n",
        "\n",
        "$w_2$ ist der Faktor vor bzw. das Gewicht von $y$ und damit $-1$.\n",
        "\n",
        "$x_0$ ist die x-Variable, die zum Gewicht $w_0$ gehört, und hat konstant den Wert 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylBAqH4jKcDo"
      },
      "source": [
        "### Lösung - Aufgabe 1.3\n",
        "Wir müssen das Gewicht $w_2$ nicht lernen, da es konstant den Wert -1 hat. In der Implementierung unten wurde auf die Vektordarstellung verzichtet und deshalb $w_2$ nicht einbezogen. \n",
        "\n",
        "Natürlich ist auch eine Lösung möglich, die $w_2$ (und $x_0 = 1$) mit einbezieht und die Vektordarstellung wie in der Vorlesung nutzt. Dabei können sie $w_2=-1$ trotzdem konstant lassen oder ebenfalls mit trainieren.\n",
        "Um für diesen Fall die resultierende lineare Funktion zu konstruieren, müssten Sie anschließend jedoch $w_1$ und $w_0$ durch $-w_2$ teilen:\n",
        "\n",
        "$0 = w_0 \\cdot 1 + w_1x + w_2y$\n",
        "$\\Leftrightarrow -w_2y = w_0*1 + w_1x$\n",
        "$\\Leftrightarrow y = \\frac{w_1}{-w_2}x + \\frac{w_0}{-w_2}$\n",
        "\n",
        "So wird gut sichtbar, dass Sie eigentlich nur zwei Gewichte lernen brauchen, um den nötigen linearen Zusammenhang zu beschreiben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znfQB0Z1HCMQ"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = [(0.44, 0.12, 0), (1.15, 0.15, 0), (1.46, 0.34, 0), (1.5, 0.35, 0), (2.35, 0.57, 0), \n",
        "        (2, 0.72, 0), (6.8, 0.91, 1), (3.98, 0.86, 1), (4.1, 0.98, 1), (4.92, 0.79, 1), \n",
        "        (5.29, 1.22, 1), (6.31, 1.29, 1), (6.09, 1.29, 1), (6.5, 1.52, 1), (7.54, 1.19, 1), \n",
        "        (8.47, 1.43, 1), (8.33, 1.36, 1), (8.75, 1.49, 1), (9.69, 2.0, 1), (9.8, 1.65, 1)]\n",
        "\n",
        "x1,x2 = 0,10\n",
        "n = len(data)\n",
        "\n",
        "def f(x):\n",
        "  return w1*x + w0\n",
        "\n",
        "# The mean squared error\n",
        "# Takes a list of the true/expected values and a list of the results obtained by our model\n",
        "def loss(ys, fs):\n",
        "  return (1.0/n) * sum([math.pow(y - f, 2) for (y,f) in zip(ys, fs)])\n",
        "\n",
        "def showplot():\n",
        "  plt.ylim(0,2.5)\n",
        "  plt.xlim(0,10)\n",
        "  plt.scatter([x for (x,y,z) in data if z],[y for (x,y,z) in data if z])\n",
        "  plt.scatter([x for (x,y,z) in data if not z],[y for (x,y,z) in data if not z])\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.show()\n",
        "\n",
        "showplot()\n",
        "\n",
        "### Gradient Descent for Linear Regression\n",
        "\n",
        "w0,w1=0.0,0.0\n",
        "epochs = 10\n",
        "alpha = 0.004\n",
        "for i in range(epochs):\n",
        "  plt.plot([x1,x2], [f(x1),f(x2)], color=(1,0,0,(0.05+0.95*(i+1)/epochs)))  # regression line\n",
        "  w0 += alpha*(2.0/n)*sum([  (y-f(x)) for (x,y,z) in data])\n",
        "  w1 += alpha*(2.0/n)*sum([x*(y-f(x)) for (x,y,z) in data])\n",
        "\n",
        "print(f\"Durch Gradientenabstieg in {epochs} Schritten gefundene Lösung\")\n",
        "print(f\"f(x)= {w1:.2f} x + {w0:.2f}\")\n",
        "print(f\"Loss: {loss([y for (x,y,z) in data],[f(x) for (x,y,z) in data]):.2f}\")\n",
        "\n",
        "showplot()\n",
        "\n",
        "### Gradient Descent to Train Linear Classifier\n",
        "def logistic(x,y):\n",
        "  return 1/(1+math.exp(-(w0+w1*x-y)))\n",
        "\n",
        "w0,w1=0.0,0.0\n",
        "epochs = 24\n",
        "alpha = 2.5\n",
        "for i in range(epochs):\n",
        "  plt.plot([x1,x2], [f(x1),f(x2)], color=(1,0,0,(0.05+0.95*(i+1)/epochs)))  # regression line\n",
        "  w0 += alpha*(2.0/n)*sum([(z-logistic(x,y))*logistic(x,y)*(1-logistic(x,y))   for (x,y,z) in data])\n",
        "  w1 += alpha*(2.0/n)*sum([(z-logistic(x,y))*logistic(x,y)*(1-logistic(x,y))*x for (x,y,z) in data])\n",
        "\n",
        "print(f\"Durch Gradientenabstieg in {epochs} Schritten gefundene Lösung:\\n f(x)= {w1:.2f} x + {w0:.2f}\")\n",
        "print(f\"Die logistische Funktion, mit deren Hilfe klassifiziert wird, ist entsprechend:\\n label(x,y)=1/(1+e^(-({w0:.2f}+{w1:.2f}*x-y)))\")\n",
        "print(f\"Loss: {loss([z for (x,y,z) in data],[logistic(x,y) for (x,y,z) in data]):.2f}\")\n",
        "\n",
        "showplot()\n",
        "\n",
        "print(f\"Label von (0,0) ist: {int(round(logistic(0,0),0))}\")\n",
        "print(f\"Label von (6,0.5) ist: {int(round(logistic(6,0.5),0))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CibsICjprrag"
      },
      "source": [
        "## Aufgabe 2 - Perzeptron\n",
        "\n",
        "Wir möchten zwei Gruppen von Punkten unterscheiden. Die Entscheidungsgrenze ist im folgenden Bild gegeben. Die roten und blauen Kreise repräsentieren dabei Beispiele mit unterschiedlichen Klassen. Bestimmen Sie anhand des Bildes den Gewichtsvektor $\\vec{w}$ mit den Gewichten $w_0$, $w_1$ und $w_2$ und den Vektor $\\vec{x}$ mit $x_0$, $x_1$ und $x_2$, wobei $x_0=1$ ist.\n",
        "\n",
        "![Entscheidungsgrenze](https://cloudstore.zih.tu-dresden.de/index.php/s/TwSsSfaxrNf4axj/download/decisionboundary-edited.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwwKGp6Q7RKY"
      },
      "source": [
        "### Lösung - Aufgabe 2\n",
        "\n",
        "Aus dem Bild erkennen wir\n",
        "\n",
        "$x_2 = 2x_1 + 2$\n",
        "\n",
        "$\\Leftrightarrow 2 + 2x_1 - x_2 = 0.$\n",
        "\n",
        "Mit \n",
        "\n",
        "$\\vec{w} = \\begin{pmatrix} 2 \\\\ 2 \\\\-1\\end{pmatrix}, \\qquad \\vec{x} = \\begin{pmatrix}x_0 \\\\ x_1\\\\ x_2\\end{pmatrix}$\n",
        "\n",
        "können wir schreiben:\n",
        "\n",
        "$\\vec{w} \\cdot \\vec{x} = 0, \\qquad für \\quad x_0=1.$\n",
        "\n",
        "Die Entscheidungsregel ergibt sich daraus wie folgt: \n",
        "\n",
        "$\\vec{w} \\cdot \\vec{x} < 0$  Rot\n",
        "\n",
        "$\\vec{w} \\cdot \\vec{x} > 0$  Blau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsAhSLVStNTH"
      },
      "source": [
        "## Aufgabe 3 - Datentransformation\n",
        "Gegeben seien Eingangsvektoren $x \\in \\mathbb{R}_{>0}^n$ (d.h. für Komponenten des Vektors $x$ gilt: $x_i > 0$). Diese Eingangsvektoren können anhand der Entscheidungsregel \n",
        "$$\\prod_i x_i^{w_i} \\gtrless b$$\n",
        "in zwei unterschiedliche Klassen aufgeteilt werden. \n",
        "\n",
        "Wie kann man anhand eines gelabelten Datensatzes $(x^m, k^m)$ der Größe M und linearer Regression die unbekannten Parameter $w_i$ und $b$ lernen?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRKLbopS9URk"
      },
      "source": [
        "### Lösung - Aufgabe 3\n",
        "\n",
        "Wir wenden $\\ln$ auf beiden Seiten der Gleichung an, also\n",
        "\n",
        "$\\sum_i w_i \\ln x_i \\gtrless \\ln b$\n",
        "\n",
        "$\\Leftrightarrow \\sum_i (w_i \\ln x_i) - \\ln b \\gtrless 0$\n",
        "\n",
        "$\\Leftrightarrow \\vec{w} \\cdot \\vec{x} \\gtrless 0$ \n",
        "\n",
        "mit\n",
        "\n",
        "$\\vec{w} = \\begin{pmatrix} w_1 \\\\ \\vdots \\\\w_n \\\\ -\\ln b \\end{pmatrix}, \\qquad \\vec{x} = \\begin{pmatrix}\\ln x_1 \\\\ \\vdots \\\\ \\ln x_n \\\\ 1 \\end{pmatrix}.$\n",
        "\n",
        "Damit haben wir ein lineares Klassifikationsproblem wie in Aufgabe 1 mit den Daten $(\\ln x^m, k^m)$ (komponentenweise Anwendung von $\\ln$ auf Vektor $x$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruL00UW-virI"
      },
      "source": [
        "## Aufgabe 4 - Logische Netzwerke\n",
        "\n",
        "Wir betrachten die boolsche Funktion\n",
        "\n",
        "$$y = (x_1 \\lor \\overline{x_2})\\land (\\overline{x_1}\\lor x_3),$$\n",
        "welche jedem Tripel $x = (x_1, x_2, x_3)$ boolscher Variablen, $x_i \\in \\{0, 1\\}$ (Input) einen boolschen Wert $y\\in \\{0, 1\\}$ (Output) zuweist.\n",
        "\n",
        "a) Konstruieren Sie ein Feed-Forward-Netzwerk mit Schwellwert-Neuronen (Neuronen mit der Stufenfunktion), welches diese Funktion implementiert. \n",
        "\n",
        "Wie viele Neuronen und wieviele Schichten werden gebraucht? \n",
        "\n",
        "Wie müssen die Neuronen verbunden werden? \n",
        "\n",
        "Wie sind die Gewichte und Schwellwerte der Neuronen?\n",
        "\n",
        "Hinweis: Konstruieren Sie die Schwellwert-Neuronen so, dass 0 oder 1 ausgegeben wird, je nachdem, ob der Eingang über oder unter dem Schwellwert liegt. Wählen Sie als Gewicht für negierte Inputs -1 und für nicht-negierte Inputs +1. Überlegen Sie sich, wie der Schwellwert bei einem logischen Und $\\land$ bzw. einem logischen Oder $\\lor$ gewählt werden muss.\n",
        "\n",
        "b) Generalisieren Sie dieses Vorgehen auf beliebige boolsche Funktionen, d.h. beliebige Funktionen $f: \\{0, 1\\}^n \\mapsto \\{0, 1\\}$.\n",
        "\n",
        "c) Argumentieren Sie ferner, dass jede boolsche Funktion nur mit einem Feed-Forward-Netzwerk mit nur einer versteckten (hidden) Schicht realisiert werden kann. \n",
        "\n",
        "Welches Netzwerk erhält man beispielsweise für die Funktion\n",
        "$$y = (x_1 \\lor \\overline{x_2}) \\land x_3$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEJwS8PKAXka"
      },
      "source": [
        "### Lösung - Aufgabe 4\n",
        "a) Konstruiere zuerst Standard-Funktionen für *Negation*, *logisches Und* $\\land$, und *logisches Oder* $\\lor$.\n",
        "\n",
        "Für *Negation* wähle Gewicht $-1$ und Schwellwert $-\\frac{1}{2}$.\n",
        "\n",
        "Für $\\lor$ wähle als Gewicht für negierte Inputs $-1$ und nicht-negierte $+1$. Als Schwellwert wähle Summe aller Gewichte negierter Inputs und addiere 0.5, also $\\sum \"-1\" + 0.5$.\n",
        "\n",
        "Mathematischer: Angenommen, wir haben eine Disjunktion $\\bigvee_i x_i^{a_i}$ für boolsche Variablen $x_i\\in \\{0, 1\\}$ und $a_i\\in \\{0, 1\\}$, wobei $x_i^0 = \\bar{x}_i$ und $x_i^1 = x_i$ ist. Wir wollen ein Schwellwert-Neuron konstruieren mit $o = f_t(\\sum_i w_i x_i)$. $f_t(x)$ ist 1, falls $\\sum_i w_i x_i > t$ und 0, falls $\\sum_i w_i x_i < t$. Oben in Worten gefasstes lässt sich formulieren als $w_i = (-1)^{1-a_i}$. Der Schwellwert $t$ ergibt sich aus $t = \\sum_{i: a_i=0} w_i + 0.5 = -\\sum_{i: a_i=0} 1 + 0.5$. \n",
        "\n",
        "Für $\\land$ wähle als Gewicht für negierte Inputs $-1$ und nicht-negierte $+1$. Als Schwellwert wähle Summe aller Gewichte nicht-negierter Inputs und subtrahiere 0.5, also $\\sum \"+1\" - 0.5$.\n",
        "\n",
        "Mathematischer: Angenommen, wir haben eine Konjunktion $\\bigwedge_i x_i^{a_i}$ für boolsche Variablen $x_i\\in \\{0, 1\\}$ und $a_i\\in \\{0, 1\\}$, wobei $x_i^0 = \\bar{x}_i$ und $x_i^1 = x_i$ ist. Wir wollen ein Schwellwert-Neuron konstruieren mit $o = f_t(\\sum_i w_i x_i)$. $f_t(x)$ ist 1, falls $\\sum_i w_i x_i > t$ und 0, falls $\\sum_i w_i x_i < t$. Oben in Worten gefasstes lässt sich formulieren als $w_i = (-1)^{1-a_i}$. Der Schwellwert $t$ ergibt sich aus $t = \\sum_{i: a_i=1} w_i - 0.5 = \\sum_{i: a_i=1} 1 - 0.5$. \n",
        "\n",
        "Damit kann man nun das Netzwerk für die gegebene Funktion aufbauen. Zuerst betrachten wir die Wahrheitstabelle. Darin repräsentieren gleiche Farben einander entsprechende Ausgänge.\n",
        "\n",
        "![Wahrheitstabelle](https://cloudstore.zih.tu-dresden.de/index.php/s/mcMXzTWfmqRtn6K/download/truthtable.png)\n",
        "\n",
        "Anschließend zeichnen wir das Netzwerk. Das Netzwerk hat die Eingänge $x_1$, $x_2$ und $x_3$. In der verborgenen Ebene befinden sich zwei Schwellwert-Neuronen mit dem Schwellwert -0.5. Diese beiden Neuronen werten die Oder $\\lor$ Verknüpfungen in den Klammern aus. In der Ausgangsebene befindet sich ein Neuron mit dem Schwellwert 1.5. Dieses Neuron wertet die Und $\\land$ Verknüpfung der geklammerten Ausdrücke aus.\n",
        "\n",
        "![Netzwerk A3a](https://cloudstore.zih.tu-dresden.de/index.php/s/HwtxzALf4max8b6/download/task3a.png)\n",
        "\n",
        "b, c) Wir können jede boolsche Funktion in konjunktive oder disjunktive Normalfunktion umwandeln (KNF oder DNF). Damit können wir in der Hidden-Schicht die Klammern auswerten und dann in der Output-Schicht die geklammerten Ausdrücke zusammenfassen. \n",
        "\n",
        "![Netzwerk A3bc](https://cloudstore.zih.tu-dresden.de/index.php/s/jBy3kofwGtAKsZG/download/task3b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkR5UPqj4aEI"
      },
      "source": [
        "## Aufgabe 5 - Anzahl lernbarer Parameter\n",
        "\n",
        "Gegeben sei ein Netzwerk mit einer Input-Schicht, einer Hidden-Schicht und einer Output-Schicht. Die Input-Schicht hat $i$ Neuronen. Die Hidden-Schicht hat $h$ Neuronen. Die Output-Schicht hat $o$ Neuronen. Die Input-Schicht sei vollständig mit der Hidden-Schicht verbunden. Die Hidden-Schicht sei vollständig mit der Output-Schicht verbunden.\n",
        "\n",
        "a) Wieviele Parameter müssen wir in diesem Netzwerk optimieren?\n",
        " Was ergibt sich für $i =4$, $h = 3$ und $o = 2$?\n",
        "\n",
        "*Hinweis:* Die Bias-Neuronen kommen zusätzlich noch hinzu. In der Output-Schicht befinden sich keine Bias Neuronen. Versuchen Sie sich zu erklären, warum das so ist.\n",
        "\n",
        "b) Zeichen Sie das Netz!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm4zHkwj_dNf"
      },
      "source": [
        "### Lösung - Aufgabe 5\n",
        "\n",
        "a) Wir haben insgesamt \n",
        "\n",
        "$$N = ih + ho + h + o = h(i+o+1) + o$$\n",
        "\n",
        "Gewichte, die wir optimieren müssen. Für die gegebenen Werte ergeben sich also \n",
        "\n",
        "$$N = 3(4+2+1) + 2 = 23$$\n",
        "Gewichte.\n",
        "\n",
        "b) \n",
        "\n",
        "![Netzwerk A4](https://cloudstore.zih.tu-dresden.de/index.php/s/jSHxcX4cE7rKSXc/download/task4.png)"
      ]
    }
  ]
}