{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent_solution.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InSuLaTi0N/Informatik/blob/master/gradient_descent_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lg7Ks482Q8c"
      },
      "source": [
        "Version: 2021.01.25\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRPJSPbQLkzC"
      },
      "source": [
        "# Intelligente Systeme - Übung Lineare Regression\n",
        "\n",
        "Ziel der linearen Regression ist es, eine lineare Funktion (oder Hypothese) zu finden, welche die Beispieldaten möglichst akkurat beschreibt und mit der sich später wiederum Vorhersagen machen lassen. Dazu wurden in der Vorlesung zwei Verfahren vorgestellt. Beide Verfahren wollen die Fehlerfunktion minimieren und verwenden dazu den Gradienten der Fehlerfunktion. Der Gradient einer Funktion $f(x,y)$ ist definiert als:\n",
        "\n",
        "$$\\nabla f(x,y) = \\begin{pmatrix}\\frac{\\partial \\mathcal{f}}{\\partial x}\\\\ \\frac{\\partial \\mathcal{f}}{\\partial y}\\end{pmatrix}$$\n",
        "\n",
        "Mit der geschlossenen Lösung haben wir ein exaktes analytisches Verfahren zur Bestimmung des Minimums betrachtet. Eine numerische Lösung findet der Gradientenabstieg, mit dessen Hilfe wir uns der analytischen Lösung immer weiter annähern. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOgbX0dLuVd"
      },
      "source": [
        "# Aufgabe 1 - Gradientenberechnung\n",
        "\n",
        "1. Berechnen Sie die Gradienten der folgenden beiden Funktionen\n",
        "\n",
        "$$f(x, y) = \\frac{1}{x^2+y^2}$$\n",
        " \n",
        "$$f(x, y) = x^2y$$\n",
        "\n",
        "2. Das Gradientenabstiegsverfahren lässt sich nicht nur im Kontext der linearen Regression nutzen, sondern ganz allgemein um Minima von Funktionen zu finden. Schreiben Sie die Update-Gleichungen der beiden Funktionen auf, die Sie nutzen würden, um auf diesem Weg ihre Minima zu bestimmen. Welche Eigenschaft hat ein Gradient und wie wird diese hier genutzt? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9Oyc6Zl8jO"
      },
      "source": [
        "## Lösung\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aICEiOepBSRF"
      },
      "source": [
        "### 1.1 Gradienten\n",
        "Für $f(x, y) = \\frac{1}{x^2+y^2}$ ergibt sich: \n",
        "\n",
        "$$\\nabla f(x, y) = \\begin{pmatrix}-\\frac{2x}{(x^2+y^2)^2} \\\\ -\\frac{2y}{(x^2+y^2)^2}\\end{pmatrix}$$\n",
        "\n",
        "Für $f(x, y) = x^2y$ ergibt sich: \n",
        "\n",
        "$$\\nabla f(x,y) = \\begin{pmatrix}2xy \\\\ x^2\\end{pmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTloxxvHBWP1"
      },
      "source": [
        "### 1.2 Update-Gleichungen\n",
        "Der Gradient einer Funktion ist ein Vektorfeld. Für jede Position im Feld zeigt der Vektor an dieser Stelle in die Richtung des steilsten Anstiegs. Um das Minimum zu finden, müssen wir uns also in die entgegengesetzte Richtung bewegen und ziehen deshalb für jede Koordinate den Wert des Gradienten an dieser Stelle ab - multipliziert mit der Schrittgröße $\\alpha$. \n",
        "\n",
        "Wenn wir uns exakt in einem Optimum befinden, so ist an dieser Stelle der Anstieg und damit der gesamte Vektor gleich $0$ und die Werte der Koordinaten werden nicht mehr verändert.\n",
        "\n",
        "Die Update-Gleichungen für $f(x, y) = \\frac{1}{x^2+y^2}$ lauten also:\n",
        "\n",
        "$$x \\leftarrow x + \\alpha \\frac{2x}{(x^2+y^2)^2}$$\n",
        "\n",
        "$$y \\leftarrow y + \\alpha \\frac{2y}{(x^2+y^2)^2}$$\n",
        "\n",
        "Die Update-Gleichungen für $f(x, y) = x^2y$ lauten:\n",
        "\n",
        "$$x \\leftarrow x - 2\\alpha xy$$\n",
        "\n",
        "$$y \\leftarrow y - \\alpha x^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4WU-6qORC9"
      },
      "source": [
        "# Aufgabe 2 - Lineare Regression\n",
        "Die folgende Tabelle gibt den Treibstoffverbrauch $c$ in $\\frac{l}{100 \\text{km}}$ bei gegebener Fahrtgeschwindigkeit $s$ in $\\frac{\\text{km}}{\\text{h}}$ wieder: \n",
        "\n",
        "|$s$|$c$|\n",
        "|--|--|\n",
        "|0|\t0|\n",
        "|30\t|3.5|\n",
        "|50|5|\n",
        "|80|6.8|\n",
        "|100|7.4|\n",
        "|130|8|\n",
        "|180|\t12|\n",
        "\n",
        "Wir wollen eine lineare Funktion finden, die den Zusammenhang beschreibt.\n",
        "\n",
        "1. Schreiben Sie eine geeignete Loss-Funktion $\\mathcal{L}(\\vec{w})$ für $n$ Datenpunkte $(s_i, c_i)$ auf. Benutzen Sie eine lineare Funktion $c(s) = w_1 s + w_0$ als Hypothese. Welche Möglichkeiten gibt es und welche Eigenschaften sind für eine Loss Funktion relevant? Warum werden nicht einfach alle Fehler aufsummiert?\n",
        "\n",
        "2. Leiten Sie für den Gradientenabstieg die Update-Gleichungen für $w_1$ und $w_0$ her. \n",
        "\n",
        "3. Vervollständigen Sie entsprechend der Update-Gleichungen den untenstehenden Code. Probieren Sie auch unterschiedliche Startwerte $w_0$ und $w_1$ aus. Was passiert für zu große Lernraten $\\alpha$, was für zu kleine Lernraten $\\alpha$?\n",
        "\n",
        "4. Bestimmen Sie durch Nullsetzen des Gradienten die optimalen $w_0$ und $w_1$ (geschlossene Lösung) und vergleichen Sie mit der numerisch ermittelten Lösung (Gradientenabstieg).\n",
        "\n",
        "5. *(Zusatz) Auch für die folgende allgemeine Hypothese $$y(x) = \\sum_{i=1}^m w_i f_i(x)$$ kann man die Loss-Funktion aufschreiben und durch Nullsetzen des Gradienten die optimalen Gewichte $w_i$ bestimmen. Versuchen Sie dies.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvl-VormA66S"
      },
      "source": [
        "## Lösung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8eKgUynoH7"
      },
      "source": [
        "### 2.1 Loss-Funktion\n",
        "Würde man einfach alle Fehler aufsummieren, könnten sich positive und negative Fehler aufheben und wir würden keine optimale Lösung bekommen. Naheliegend wäre die Summe der Beträge aller Fehler zu betrachten. Um in jedem Punkt den Gradienten bestimmen zu können, muss die Funktion jedoch in jedem Punkt differenzierbar sein. In dem wir den Fehler quadrieren, lösen wir beide Probleme, wir bekommen eine differenzierbare Funktion und alle Fehler gehen positiv in die Rechnung ein.\n",
        "Auf den Vorlesungsfolien ist Ihnen die Sum of Squared Errors (SSE) als Loss-Funktion begegnet, die für unser Problem so aussieht:\n",
        "\n",
        "$$\\mathcal{L_{SSE}}(\\vec{w}) = \\sum_{i=1}^n (c_i - (w_1 s_i + w_0))^2$$\n",
        "\n",
        "Diese Loss-Funktion hat jedoch den Nachteil, dass auch bei sehr kleinen Fehlern pro Datenpunkt die Funktionswerte mit der Zahl der Daten immer größer werden. Deshalb ist es üblich, anstelle des SSE den MSE (Mean Squared Error) zu verwenden und das Ergebnis noch durch die Zahl der Datensätze zu teilen:\n",
        "\n",
        "$$\\mathcal{L_{MSE}}(\\vec{w}) = \\frac{1}{n}\\sum_{i=1}^n (c_i - (w_1 s_i + w_0))^2$$\n",
        "\n",
        "Der MSE ist Ihnen bereits im Code zur Vorlesung begegnet.\n",
        "\n",
        "In einer Variante des MSE Losses wird zusätzlich mit dem Faktor $1/2$ multipliziert - das hat den Hintergrund, dass so der Faktor $2$, der beim Ableiten entsteht, verschwindet. \n",
        "\n",
        "$$\\mathcal{L}(\\vec{w}) = \\frac{1}{2n}\\sum_{i=1}^n (c_i - (w_1 s_i + w_0))^2$$\n",
        "\n",
        "$$= \\frac{1}{2n}\\sum_{i=1}^n (c_i - w_1 s_i - w_0)^2$$\n",
        "\n",
        "\n",
        "Mit dieser Variante werden wir in dieser Aufgabe weiterarbeiten. Auf die Position des Minimums hat die Multiplikation mit den Konstanten $1/n$ und $1/2$ natürlich keinen Einfluss und wenn Sie mit nur einem oder keinem der beiden konstanten Faktoren weitergearbeitet haben, **kann Ihre Lösung in den folgenden Teilaufgaben abweichen aber trotzdem richtig sein**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQCsZQF8AtWS"
      },
      "source": [
        "### 2.2 Update-Gleichungen\n",
        "Die partiellen Ableitungen von $\\mathcal{L}$ sind \n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial w_0} = - \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0)$$ \n",
        "\n",
        "und \n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial w_1} = - \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0) s_i$$ \n",
        "\n",
        "Für die Update-Gleichungen ergeben sich entsprechend\n",
        "\n",
        "$$w_0 \\leftarrow w_0 - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_0} = w_0 + \\alpha \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0)$$ und $$w_1 \\leftarrow w_1 - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_1} = w_1 + \\alpha \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0)s_i$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZu686WyA_Tp"
      },
      "source": [
        "### 2.3 Implementierung und Initialisierung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiWK0etTLhPj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def update(w1, w0, alpha, s, c):\n",
        "  n = len(s)\n",
        "  dw1 = 1/n*np.sum((w0 + w1*s - c)*s)\n",
        "  dw0 = 1/n*np.sum(w0 + w1*s - c)\n",
        "\n",
        "  w1 = w1 - alpha*dw1\n",
        "  w0 = w0 - alpha*dw0\n",
        "\n",
        "  return w1, w0\n",
        "\n",
        "\n",
        "s = np.array([0, 30, 50, 80, 100, 130, 180])\n",
        "c = np.array([0, 3.5, 5.0, 6.8, 7.4, 8.0, 12.0])\n",
        "\n",
        "iterations = 100\n",
        "\n",
        "# Startwerte\n",
        "w1 = 2\n",
        "w0 = 2\n",
        "\n",
        "# Lernrate\n",
        "alpha = 0.0001\n",
        "\n",
        "for i in range(iterations):\n",
        "  w1, w0 = update(w1, w0, alpha, s, c)\n",
        "\n",
        "print(f\"Numerische Lösung: c = {w1:.2f}s + {w0:.2f}\")\n",
        "\n",
        "# Optimale Lösung (Aufgabe 2.4)\n",
        "bestw1 = (np.mean(c*s) - np.mean(c)*np.mean(s))/(np.mean(s**2) - np.mean(s)**2)\n",
        "bestw0 = np.mean(c) - bestw1*np.mean(s)\n",
        "\n",
        "print(f\"Optimale Lösung: c = {bestw1:.2f}s + {bestw0:.2f} (grün)\")\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel(r\"$s/\\frac{km}{h}$\")\n",
        "plt.ylabel(r\"$c/\\frac{l}{100km}$\")\n",
        "plt.plot(s, c, '.')\n",
        "plt.plot(s, s*w1 + w0)\n",
        "plt.plot(s, s*bestw1 + bestw0, color=\"green\") # Optimale Lösung (Aufgabe 2.4)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDi7p3KWAwib"
      },
      "source": [
        "### 2.4 Geschlossene Lösung\n",
        "\n",
        "Wir setzen $<x> = \\frac{1}{n}\\sum_{i=1}^n x_i$. \n",
        "\n",
        "Nullsetzen der partiellen Ableitungen ergibt dann unter Nutzung dieser Abkürzung\n",
        "\n",
        "$$0 = \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0) = <c> - w_1 <s> - w_0$$\n",
        "\n",
        "$$0 = \\frac{1}{n}\\sum_{i=1}^n (c_i - w_1s_i -w_0) s_i = <cs> - w_1 <s^2> - w_0 <s>$$\n",
        "\n",
        "Die erste Gleichung stellen wir nach $w_0$ um:\n",
        "\n",
        "$$w_0 = <c> - w_1 <s>$$\n",
        "\n",
        "Das Ergebnis setzen wir in der zweiten Gleichung ein und stellen diese nach $w_1$ um und erhalten:\n",
        "\n",
        "$$w_1 = \\frac{<cs> - <c><s>}{<s^2> - <s>^2}$$\n",
        "\n",
        "\n",
        "*(Berechnung und Vergleich zur numerischen Lösung finden sich in der Code-Zelle zur Lösung 2.3)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL71NxZvAzaV"
      },
      "source": [
        "### 2.5 (Zusatz) Allgemeine Hypothese\n",
        "Die Loss-Funktion ist\n",
        "\n",
        "$$\\mathcal{L} = \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\sum_{j=0}^m w_j f_j(x_i)\\right)^2$$\n",
        "\n",
        "Ableiten nach beliebigem $w_k$ ergibt \n",
        "\n",
        "$$\\frac{\\mathcal{L}}{\\partial w_k} = -\\frac{1}{n} \\sum_{i=1}^n\\left(y_i - \\sum_{j=0}^m w_j f_j(x_i)\\right)f_k(x_i) = - <yf_k(x)> + \\sum_{j=0}^m w_j <f_j(x)f_k(x)> $$\n",
        "\n",
        "bzw. das Gleichungssystem\n",
        "\n",
        "$$b_k = \\sum_{j=0}^m w_ja_{jk}$$\n",
        "mit $b_k = <yf_k(x)>$ und $a_{jk} = <f_j(x)f_k(x)> = a_{kj}$. \n",
        "\n",
        "Dieses kann mit Standardlösungsverfahren gelöst werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyslSSr6WONw"
      },
      "source": [
        "# Aufgabe 3 - Visualisierung Gradientenabstiegsverfahren\n",
        "\n",
        "Für die folgende Aufgabe verwenden wir das Doppelmuldenpotential \n",
        "\n",
        "$$V(x) = ax^4 + bx^2 + cx + d$$\n",
        "\n",
        "mit $a = 1$, $b = -3$, $c =1$ und $d = 3.514$. \n",
        "\n",
        "Wir wollen mithilfe des Gradientenabstiegsverfahren das globale Minimum $x_{min}$ dieser Funktion ermitteln. Sie können sich vorstellen, dass $V$ eine Loss-Funktion mit nur einem Gewicht $x$ beschreibt. \n",
        "\n",
        "1. Berechnen Sie die Ableitung und Update-Gleichung für das Gewicht $x$ mit Lernrate $\\alpha$.\n",
        "\n",
        "2. Vervollständigen Sie entsprechend unten stehenden Code.\n",
        "\n",
        "3. Testen Sie die folgenden Kombinationen für Startwert und Lernrate $(x_0, \\alpha)$. \n",
        "$$(x_0, \\alpha) = (-1.75, 0.001)$$\n",
        "$$(x_0, \\alpha) = (-1.75, 0.19)$$\n",
        "$$(x_0, \\alpha) = (-1.75, 0.1)$$\n",
        "$$(x_0, \\alpha) = (-1.75, 0.205)$$\n",
        "\n",
        "4. Wie kann man einen Kompromiss zwischen $(x_0, \\alpha) = (-1.75, 0.001)$ und $(x_0, \\alpha) = (-1.75, 0.19)$ schaffen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaIbuB2Bsmh0"
      },
      "source": [
        "## Lösung\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy00dNiOMMIc"
      },
      "source": [
        "### 3.1 Ableitung und Update-Gleichung\n",
        "Die Ableitung ist: \n",
        "\n",
        "$$\\partial_x V(x) = 4ax^3 + 2bx + c$$\n",
        "\n",
        "Die Update-Gleichung ist entsprechend:\n",
        "\n",
        "$$x \\leftarrow x - \\alpha \\left(4ax^3 + 2bx + c\\right)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf6UqLJ2MTNY"
      },
      "source": [
        "### 3.2 Implementierung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93Sqk6DGgLK"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def update2(x, a, b, c, d, alpha):\n",
        "  x = x - alpha*(4*a*x**3 + 2*b*x + c)\n",
        "\n",
        "  return x\n",
        "\n",
        "def V(x, a, b, c, d):\n",
        "  return a*x**4 + b*x**2 + c*x + d\n",
        "\n",
        "# TODO: Change to right parameters.\n",
        "a = 1\n",
        "b = -3\n",
        "c = 1\n",
        "d = 3.514\n",
        "\n",
        "x0 = -1.75\n",
        "iterations = 101\n",
        "alphas = np.array([0.001, 0.19, 0.1, 0.205])\n",
        "\n",
        "losses = np.empty(shape=(iterations, len(alphas)))\n",
        "results = np.empty(len(alphas))\n",
        "\n",
        "for j in range(len(alphas)):\n",
        "  x = x0\n",
        "  alpha = alphas[j]\n",
        "  for i in range(iterations):\n",
        "    losses[i, j] = V(x, a, b, c, d)\n",
        "    if i != iterations - 1:\n",
        "      x = update2(x, a, b, c, d, alpha)\n",
        "  results[j] = x\n",
        "\n",
        "for j in range(len(alphas)):\n",
        "  print(100*\"-\")\n",
        "  print(\"Alpha: \", alphas[j])\n",
        "  print(\"xmin: \", results[j])\n",
        "  print(\"Loss: \", V(results[j], a, b, c, d))\n",
        "\n",
        "colors = {\n",
        "    0.001: \"blue\",\n",
        "    0.19: \"red\",\n",
        "    0.1: \"black\",\n",
        "    0.205: \"orange\"\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Lernkurven\")\n",
        "plt.xlabel(\"Epoche\")\n",
        "plt.ylabel(\"Loss V\")\n",
        "plt.xlim(0, iterations)\n",
        "\n",
        "for i in range(len(alphas)):\n",
        "  alpha = alphas[i]\n",
        "  plt.plot(range(iterations), losses[:, i], label=str(alpha), color=colors[alpha])\n",
        "\n",
        "plt.legend()\n",
        "plt.ylim(bottom=0)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Funktion V und Minima\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"V(x)\")\n",
        "\n",
        "xs = np.linspace(-2, 2, 100)\n",
        "ys = V(xs, a, b, c, d)\n",
        "\n",
        "plt.plot(xs, ys)\n",
        "\n",
        "for j in range(len(alphas)):\n",
        "  alpha = alphas[j]\n",
        "  xmin = results[j]\n",
        "  vxmin = V(xmin, a, b, c, d)\n",
        "  plt.plot(xmin, vxmin, marker='.', linestyle=\"None\", label=str(alpha), color=colors[alpha], ms=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9xgNsxtMOsn"
      },
      "source": [
        "### 3.3 Lernrate \n",
        "\n",
        "$(x_0, \\alpha) = (-1.75, 0.001)$: Linkes (globales) Minimum wird extrem langsam gefunden ($\\alpha$ zu klein).\n",
        "\n",
        "$(x_0, \\alpha) = (-1.75, 0.19)$: Kein Minimum wird gefunden. Parameter $x$ springt hin und her in der linken Mulde ($\\alpha$ zu groß).\n",
        "\n",
        "$(x_0, \\alpha) = (-1.75, 0.1)$: Linkes Minimum wird gefunden.\n",
        "\n",
        "$(x_0, \\alpha) = (-1.75, 0.205)$: Linkes Minimum wird überschossen. Rechtes lokales Minimum wird gefunden.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIfPbvM9MRfp"
      },
      "source": [
        "### 3.4 Dynamische Lernrate\n",
        "\n",
        "Passe $\\alpha$ an. Starte mit großem $\\alpha$ und reduziere z.B. alle $n$ Epochen um einen Faktor $f$.\n",
        "\n",
        "$\\alpha \\leftarrow f\\cdot\\alpha$ alle $n$ Epochen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6TVwaiMPFLW"
      },
      "source": [
        "# Aufgabe 4 - Zusatz\n",
        "\n",
        "Im [Colab zu Vorlesung](https://colab.research.google.com/drive/1W4cLIHjk1uKgf2qXmegOPVqD0BzpvhLf?usp=sharing#scrollTo=hlnSECCdse_b) wird mittels Gradientenabstieg der funktionale Zusammenhang zwischen Wohnungsgröße und Preis ermittelt. Erhöht man die Zahl der Epochen (z.B. auf 100) und lässt den Code immer wieder laufen, biegt der Gradientenabstieg, in der Talsohle angekommen, mal nach rechts, mal nach links ab. Die gesamte Talsohle scheint optimal zu sein. Denken Sie darüber nach, wovon es tatsächlich abhängt, in welche Richtung abgebogen wird. Sie können auch den Code verändern, um ihre Gedanken zu überprüfen und die geschlossene Lösung im Code ergänzen.\n",
        "\n",
        "Hinweise:\n",
        "- Welche Bedeutung haben die beiden Parameter $w_0$ und $w_1$? **Achtung: Die Parameter $w_0$ und $w_1$ wurden im 3D-Plot im Colab versehentlich vertauscht, dieser Fehler hat auch seinen Weg in die Folien gefunden.**\n",
        "- Was ist zu erwarten, wenn man die Menge der Trainingsdaten, also die Zahl der zufällig generierten Wohnungs-Preis-Paare, sehr viel größer wählt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZqGFwfAPHdQ"
      },
      "source": [
        "## Lösung\n",
        "\n",
        "Bei einer wachsenden Zahl an Trainingsdaten würde $w_0$ sich $0$ annähern - schließlich sollte eine nicht vorhandene Wohnung mit $0$ $m^2$ auch nichts kosten (aber auch nicht dafür sorgen, dass man Geld abgeben muss). Da wir aber mit relativ wenigen Trainingsdaten arbeiten, die immer wieder neu zufällig generiert werden, weicht der Faktor mal weniger und mal stärker nach oben oder unten ab.\n",
        "\n",
        "Um den Wert für $w_0$ zu erreichen, für den das Loss für die aktuellen Trainingsdaten minimal wird, müsste der Gradientenabstieg eine viel größere Epochenzahl haben, da der Anstieg so gering ist, dass wir einen Wohnungskaufpreis von mehreren 100.000 Euro nur um wenige Euro oder Cent pro Schritt verändern. Sichtbar wird das, indem wir für den 3D-Plot einen viel größeren Wertebereich für $w_0$ betrachten.\n",
        "\n",
        "Der untenstehende Code ergänzt den Code aus der Vorlesung um die geschlossene Lösung und einen 3D-Plot, der etwas rauszoomt und das Minimum sichtbar macht. Außerdem ist es möglich, die Zahl der Trainingsdaten deutlich größer zu wählen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wiDCE4dPGYV"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num = 50\n",
        "sizes = [random.randrange(50, 200) for i in range(num)]\n",
        "prices = [size*(1000+offset) for (size,offset) in zip(sizes, [random.randrange(1000, 3000) for i in range(num)])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oN9JPvwbvy9"
      },
      "source": [
        "def f(x):\n",
        "  return w1*x + w0\n",
        "\n",
        "def loss():\n",
        "  return (1.0/n) * sum([math.pow(y-f(x), 2) for (x,y) in zip(sizes, prices)]) \n",
        "\n",
        "n = len(sizes)\n",
        "\n",
        "bestw1 = (n*sum([ (x*y) for (x,y) in zip(sizes, prices)]) - sum(sizes)*sum(prices))/(n*sum([ (x*x) for x in sizes]) - sum(sizes)*sum(sizes))\n",
        "bestw0 = (sum(prices)-bestw1*sum(sizes))/n\n",
        "bestloss = (1.0/n) * sum([math.pow(y-(bestw1*x + bestw0), 2) for (x,y) in zip(sizes, prices)])\n",
        "print(\"Geschlossene Lösung:\")\n",
        "print(f\"f(x)= {bestw1:.2f} x + {bestw0:.2f}\")\n",
        "print(f\"Loss: {bestloss:.2f}\")\n",
        "\n",
        "w0,w1=0.0,0.0\n",
        "epochs = 100\n",
        "alpha = 0.00001\n",
        "x1,x2 = min(sizes), max(sizes)\n",
        "l=[]\n",
        "for i in range(epochs):\n",
        "  plt.plot([x1,x2], [f(x1),f(x2)], color='red')  # regression line\n",
        "  l.append((w0,w1,int(loss()/1000000000)))\n",
        "  w0 += alpha*(2.0/n)*sum([  (y-f(x)) for (x,y) in zip(sizes, prices)])\n",
        "  w1 += alpha*(2.0/n)*sum([x*(y-f(x)) for (x,y) in zip(sizes, prices)])\n",
        "\n",
        "print(f\"Durch Gradientenabstieg in {epochs} Schritten gefundene Lösung\")\n",
        "print(f\"f(x)= {w1:.2f} x + {w0:.2f}\")\n",
        "print(f\"Loss: {loss():.2f}\")\n",
        "\n",
        "plt.scatter(sizes, prices)\n",
        "plt.plot([x1,x2], [bestw1*x1+bestw0,bestw1*x2+bestw0], color=\"green\") \n",
        "plt.ylabel(\"Flat Price in EUR\")\n",
        "plt.xlabel(\"Flat Size in $m^2$\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([x1,x2], [f(x1),f(x2)], color='red')  # regression line\n",
        "plt.scatter(sizes, prices) \n",
        "plt.plot([x1,x2], [bestw1*x1+bestw0,bestw1*x2+bestw0], color=\"green\") \n",
        "plt.ylabel(\"Flat Price in EUR\")\n",
        "plt.xlabel(\"Flat Size in $m^2$\")\n",
        "plt.show()\n",
        "\n",
        "l2=[]\n",
        "for w0 in range(0,50,10):\n",
        "  for w1 in range(1000,5000,100):\n",
        "    l2.append([w0,w1,int(loss()/1000000000)])\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "w0s,w1s,losss = zip(*l)\n",
        "ax.scatter(w1s, w0s, losss, label=\"Gradient descent\", marker=\"o\")\n",
        "w0s,w1s,losss = zip(*l2)\n",
        "ax.scatter(w1s, w0s, losss,  marker=\".\")\n",
        "ax.set_xlabel('$w_1$')\n",
        "ax.set_ylabel('$w_0$')\n",
        "ax.set_zlabel('Loss in $10^9$', rotation=90)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "l3=[]\n",
        "for w0 in range(-150000,150000,5000):\n",
        "  for w1 in range(500,5000,500):\n",
        "    l3.append([w0,w1,int(loss()/1000000000)])\n",
        "for w0 in range(-150000,150000,50000):\n",
        "  for w1 in range(500,5000,50):\n",
        "    l3.append([w0,w1,int(loss()/1000000000)])\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "w0s,w1s,losss = zip(*l)\n",
        "ax.scatter(w1s, w0s, losss, label=\"Gradient descent\", marker=\"o\")\n",
        "w0s,w1s,losss = zip(*l3)\n",
        "ax.scatter(w1s, w0s, losss,  marker=\".\")\n",
        "ax.set_xlabel('$w_1$')\n",
        "ax.set_ylabel('$w_0$')\n",
        "ax.set_zlabel('Loss in $10^9$', rotation=90)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-PYyoxkb2B-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}