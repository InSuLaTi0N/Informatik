{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "name": "decision_trees_solution.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InSuLaTi0N/Informatik/blob/master/decision_trees_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XO_ksRQaGHz"
      },
      "source": [
        "Version: 2021.11.19.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76k0MaK2sz7C"
      },
      "source": [
        "# Intelligent Systems - Decision Tree Learning\n",
        "\n",
        "Aus der Vorlesung kennen wir die Formeln für die Restentropie (R), die Entropie (H) und den Information Gain (IG):\n",
        "\n",
        "&nbsp;\n",
        "$$R(T)= \\sum_{i=1}^{\\nu} \\frac{|T_i|}{|T|} \\cdot H(T_i)$$ \n",
        "&nbsp;\n",
        "$$H(T_i) = \\sum_{j=1}^{C} -p_j \\cdot log_2(p_j)$$ \n",
        "&nbsp;\n",
        "$$IG = H(T) - R(T)$$ \n",
        "&nbsp;\n",
        "\n",
        "Die Restentropie ergibt sich also aus den gewichteten Entropien der Teilmengen der Beispielmenge, die sich durch die Aufteilung nach den Werten $1...\\nu$ des gewählten Attributes ergeben. Die Entropie einer solchen Teilmenge wiederum ist abhänig vom Anteil der Bespiele in der Teilmenge, die für die Entscheidungen $1...C$ sprechen.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Rufen Sie sich auch den Pseudocode für das Anlernen eines Decision Tree Models ins Gedächtnis:\n",
        "![DTL](https://docs.google.com/uc?id=1U6sPhZkVc0VAykERjN0bJ3ImWU-zNtge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-nOeJSUKlbr"
      },
      "source": [
        "# Aufgabe 1 - Vergleich von DTMs\n",
        "\n",
        "Betrachten Sie ein Datenset aus $400$ Beispielen der Klasse $C_1$ und $400$ Beispielen der Klasse $C_2$.\n",
        "Nehmen Sie an, ein Decision Tree Model (DTM) $A$ teilt diese Beispiele in $(300,100)$ für den ersten Blattknoten und $(100,300)$ für den zweiten Blattknoten, wobei $(n,m)$ bedeutet, dass $n$ Beispiele zu $C_1$ und $m$ Beispiele zu $C_2$ gehören.\n",
        "Ein DTM $B$ teilt die Beispiele in $(200,400)$ und $(200,0)$.\n",
        "\n",
        "1. Berechnen Sie für beide DTMs den Anteil der Beispiele, die falsch klassifiziert werden (Misclassification Rate).\n",
        "\n",
        "2. Berechnen Sie den Information Gain für beide DTMs. Sie können die untenstehenden Code-Zellen als Taschenrechner verwenden. \n",
        "\n",
        "3. Gegeben seien Trainingsdaten, die aus Beispielen für alle möglichen Kombinationen zweier binärer Attribute $A$ und $B$ bestehen. Die Klasse (das Label) für jedes Beispiel ergibt sich aus der XOR-Funktion auf $A$ und $B$.\n",
        "Wie sieht ein mit diesen Trainingsdaten trainierter Decision Tree aus? Der Baum wird anschließend zum Klassifizieren ungelabelter Daten (d.h. Klasse ist nicht bekannt) mit allen möglichen Werten für $A$ und $B$ verwendet. Die auf diese Weise klassifizierten Daten werden wiederum für das Trainieren eines neuen Decision Trees genutzt. Erhalten wir den gleichen Decision Tree?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kWVDkKVE9g6"
      },
      "source": [
        "## Lösung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlXu3-55E7Kd"
      },
      "source": [
        "### 1.1 Misclassification Rates\n",
        "Für einen Blattknoten wird immer die Klasse vorhergesagt, welche die Majorität in diesem Knoten hat. Für leere Knoten wird die Majorität des Elternknotens verwendet.\n",
        "\n",
        "**DTM $A$**\n",
        "\n",
        "Von den $800$ Beispielen werden im ersten und zweiten Blattknoten jeweils $100$ Beispiele und damit insgesamt $100+100=200$ Beispiele falsch klassifiziert. Daher ist die Misclassification Rate $25\\%$.\n",
        "\n",
        "**DTM $B$**\n",
        "\n",
        "Von den $800$ Beispielen werden im ersten Blattknoten $200$ und im zweiten Blattknoten keine, also insgesamt $200+0=200$ Beispiele falsch klassifiziert. Daher ist die Misclassification Rate $25\\%$.\n",
        "\n",
        "Beide Raten sind gleich. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd8MkLjYsz7F"
      },
      "source": [
        "### 1.2 Information Gain\n",
        "\n",
        "**DTM $A$:**\n",
        "\n",
        "Entropie:\n",
        "Da die Beispiele gleichverteilt sind,\n",
        "\n",
        "$$H(T) = -0.5 \\cdot log_2(0.5)+-0.5 \\cdot log_2(0.5)) = 1$$\n",
        "\n",
        "Restentropie:\n",
        "\n",
        "Der erste Blattknoten hält den Anteil $0.5$ der Beispiele mit Wahrscheinlichkeiten $(0.75, 0.25)$.\n",
        "\n",
        "Der zweite Blattknoten hält den Anteil $0.5$ der Beispiele mit Wahrscheinlichkeiten $(0.25, 0.75)$.\n",
        "\n",
        "$$R= \\left(0.5 \\cdot \\left(-0.25 \\cdot log_2(0.25)+-0.75 \\cdot log_2(0.75)\\right)\\right) + (0.5 \\cdot \\left(-0.25 \\cdot log_2(0.25)+-0.75 \\cdot log_2(0.75)\\right))$$\n",
        "$$R= -0.25 \\cdot log_2(0.25)+-0.75 \\cdot log_2(0.75) $$\n",
        "$$R \\approx 0.81$$\n",
        "\n",
        "\n",
        "Information Gain:\n",
        "$$IG=1-0.81 = 0.19$$\n",
        "\n",
        "**DTM $B$:** \n",
        "\n",
        "Entropie:\n",
        "Da die Beispiele gleichverteilt sind,\n",
        "\n",
        "$$H(T) = -0.5 \\cdot log_2(0.5)+-0.5 \\cdot log_2(0.5)) = 1$$\n",
        "\n",
        "Restentropie:\n",
        "\n",
        "Der erste Blattknoten hält den Anteil $0.75$ der Beispiele mit Wahrscheinlichkeiten $(\\frac{1}{3}, \\frac{2}{3})$.\n",
        "\n",
        "Der zweite Blattknoten hält den Anteil $0.25$ der Beispiele mit Wahrscheinlichkeiten $(1.00, 0.00)$.\n",
        "\n",
        "$$R= (0.75 \\cdot \\left(-\\frac{1}{3} \\cdot log_2(\\frac{1}{3})+-\\frac{2}{3} \\cdot log_2(\\frac{2}{3})\\right)) + (0.25 \\cdot \\left(-1 \\cdot log_2(1)+-0 \\cdot log_2(0)\\right)$$\n",
        "\n",
        "$$R= 0.75 \\cdot \\left(-\\frac{1}{3} \\cdot log_2(\\frac{1}{3})+-\\frac{2}{3} \\cdot log_2(\\frac{2}{3})\\right) + 0.0$$\n",
        "\n",
        "$$R\\approx 0.69$$\n",
        "\n",
        "Information Gain:\n",
        "$$IG=1-0.69 = 0.31$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGKoPxpQsz7I"
      },
      "source": [
        "from math import log2\n",
        "ra = -0.25*log2(0.25) + -0.75*log2(0.75)\n",
        "iga = 1 - ra\n",
        "\n",
        "print(\"Remaining Entropy R for tree A: \", ra)\n",
        "print(\"Information Gain for tree A: \", iga)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewszVKvnsz7S"
      },
      "source": [
        "rb = 0.75 * ( -1/3*log2(1/3) + -2/3*log2(2/3) )\n",
        "igb = 1 - rb\n",
        "print(\"Remaining Entropy R for tree B\", rb)\n",
        "print(\"Information Gain for tree B: \", igb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVBQctXjyfiy"
      },
      "source": [
        "### 1.3 Decision Tree zur Klassifikation \n",
        "\n",
        "Die ursprünglichen Trainingsdaten sehen folgendermaßen aus (C ist die Klasse):\n",
        "\n",
        "A | B | C  \n",
        "--|---|---\n",
        "1 | 1 | 0 \n",
        "1 | 0 | 1\n",
        "0 | 1 | 1 \n",
        "0 | 0 | 0\n",
        "\n",
        "Wir haben also keinen Informationsgewinn beim ersten Attribut, egal welches wir wählen: der Zufall (Implementierung) entscheidet, welches zuerst betrachtet wird. Eine möglicher Baum ist:\n",
        "\n",
        "```\n",
        "                 A == 0?\n",
        "\n",
        "           /                \\\n",
        "       ja /                  \\ nein\n",
        "         /                    \\\n",
        "\n",
        "     B == 0?                 B == 0?\n",
        "\n",
        "    /      \\                /      \\\n",
        "ja /        \\ nein      ja /        \\ nein\n",
        "  /          \\            /          \\\n",
        "C=0          C=1        C=1           C=0 \n",
        "```\n",
        "\n",
        "Wird dieser Baum nun, wie in der Aufgabenstellung beschrieben, zur Klassifikation verwendet, wird die Klassenzuordnung wieder der XOR-Funktion entsprechen, da ja alle Attributkombinationen bereits im ersten Teil gegeben waren. Ein mit auf dieser Weise klassifizierten Daten trainierter Baum kann jetzt anders strukturiert sein ($\\rightarrow$ Attribut B kann zuerst gewählt werden). An der Funktion des Baumes ändert sich aber nichts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG76tMZgsz7Z"
      },
      "source": [
        "# Aufgabe 2 - Recursive Split Algorithm\n",
        "\n",
        "Auf der Basis der Daten der amerikanischen Volkszählung wollen wir die folgenden zwei Klassen bestimmen.\n",
        "\n",
        "    >50K$, <=50K$\n",
        "\n",
        "Vervollständigen Sie dazu den untenstehenden Code.\n",
        "\n",
        " **Es gibt Blanks in den folgenden Funktionen**: \n",
        "- calc_entropy()\n",
        "- calc_ig()\n",
        "- majority()\n",
        "- choose_best_attr()\n",
        "- dtree_learning()\n",
        "- dtree_classify()\n",
        "- dtree_test()\n",
        "\n",
        "**Die Attribute sind**: \n",
        "\n",
        "* **age**: continuous.  \n",
        "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n",
        "* **fnlwgt**: continuous. (= final weight)\n",
        "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "* **education-num**:  continuous. \n",
        "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n",
        "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n",
        "* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. \n",
        "(Comment: this term is (was?) quite common in the american litrature, although ...  search the net for the debate in the U.S. and elsewhere.) \n",
        "* **sex**: Female, Male.  \n",
        "* **capital-gain**: continuous.\n",
        "* **capital-loss**: continuous.\n",
        "* **capital-loss**: continuous.\n",
        "* **capital-loss**: continuous.\n",
        "* **hours-per-week**: continuous.\n",
        "* **native-country**:  United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
        "\n",
        "Wir benutzen keine kontinuierlichen Attribute. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rckV2qvOzOt"
      },
      "source": [
        "**Upload der Dateien für Google-Collaboratory**\n",
        "\n",
        "Wir verwenden wget, um die benötigten Dateien hier in Google-Collaboratory verfügbar zu machen. Wget ist ein Programm, mit dem Dateien von HTTP-, HTTPS-, FTP- und FTPS-Servern abgerufen werden können. Wenn Sie ein Linux- oder Mac-Benutzer sind, ist wget normalerweise in der Standardinstallation enthalten. \n",
        "\n",
        "Hier im Colab can wget unabhängig vom Betriebssystem genutzt werden!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg9oEOonJvfJ"
      },
      "source": [
        "!wget https://cloudstore.zih.tu-dresden.de/index.php/s/gxBP63LNy9SgNFD/download -O adult.data.txt\n",
        "!wget https://cloudstore.zih.tu-dresden.de/index.php/s/yw5ayzYk9g2bc3p/download -O adult.test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt5QJHShO25j"
      },
      "source": [
        "Alternativ können Sie wie folgt vorgehen, um die Dateien verfügbar zu machen (hierzu müssen Sie in Ihren Google-Account eingeloggt sein):\n",
        "\n",
        "1.   Speichern Sie die Dateien [*adult.data.txt*](https://sharing.crt-dresden.de/index.php/s/5Jo4xAZdpCW9uIM/download) und [*adult.test.txt*](https://sharing.crt-dresden.de/index.php/s/CZoJ0YdrsSmdOui/download) auf Ihrem Gerät\n",
        "2.   Öffnen Sie die Navigation des Notebooks, und laden Sie die Dateien in Ihre Runtime hoch (Files, Upload)\n",
        "3. Anschließend können Sie auf diese Dateien wie gewohnt aus Ihrem Python-Code zugreifen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHI3Q9m5JwgV"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb5295N1FpLD"
      },
      "source": [
        "##Lösung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm1GxQa0sz7f"
      },
      "source": [
        "\"\"\" Intelligent Systems TUD 2020, Ex.1\n",
        "\n",
        "Decision Tree Learning:\n",
        "Based on american census data you want to predict two classes of income of people:\n",
        ">50K$, <=50K$.\n",
        "\n",
        "We do not use continuous attributes for this first decision tree task.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "__author__ = 'Benjamin Guthier'\n",
        "\n",
        "from math import log\n",
        "\n",
        "\n",
        "def openfile(path, fname):\n",
        "    \"\"\"opens the file at path+fname and returns a list of examples and attribute values.\n",
        "    examples are returned as a list with one entry per example. Each entry then\n",
        "    is a list of attribute values, one of them being the class label. The returned list attr\n",
        "    contains one entry per attribute. Each entry is a list of possible values or an empty list\n",
        "    for numeric attributes.\n",
        "    \"\"\"\n",
        "    datafile = open(path + fname, \"r\")\n",
        "    examples = []\n",
        "    for line in datafile:\n",
        "        line = line.strip()\n",
        "        line = line.strip('.')\n",
        "        # ignore empty lines. comments are marked with a |\n",
        "        if len(line) == 0 or line[0] == '|':\n",
        "            continue\n",
        "        ex = [x.strip() for x in line.split(\",\")]\n",
        "        examples.append(ex)\n",
        "    attr = []\n",
        "    for i in range(len(examples[0])):\n",
        "        values = list({x[i] for x in examples})  # set of all different attribute values\n",
        "        if values[0].isdigit():  # if the first value is a digit, assume all are numeric\n",
        "            attr.append([])\n",
        "        else:\n",
        "            attr.append(values)\n",
        "\n",
        "    return examples, attr\n",
        "\n",
        "\n",
        "def calc_entropy(examples, cls_index):\n",
        "    \"\"\"calculates the entropy over all examples. The index of the class label in the example\n",
        "    is given by cls_index. Can also be the index to an attribute.\n",
        "    \"\"\"\n",
        "    global attr\n",
        "    # get attributes of examples with index cls_index\n",
        "    example_classifications = [example[cls_index] for example in examples]\n",
        "    # get unique counts of example_attributes\n",
        "    unique, counts = np.unique(example_classifications, return_counts=True)\n",
        "\n",
        "    # normalize counts to total number of examples for getting probs\n",
        "    probs = counts/len(examples)\n",
        "    # print(probs)\n",
        "\n",
        "    entropy = -np.sum(probs*np.log2(probs))\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def calc_ig(examples, attr_index, cls_index):\n",
        "    \"\"\"Calculates the information gain over all examples for a specific attribute. The\n",
        "    class index must be specified.\n",
        "\n",
        "    uses calc_entropy\n",
        "    \"\"\"\n",
        "    global attr\n",
        "    # get attributes of examples with index attr_indx\n",
        "    example_attributes = [example[attr_index] for example in examples]\n",
        "\n",
        "    # get unique counts of example_attributes\n",
        "    unique, counts = np.unique(example_attributes, return_counts=True)\n",
        "\n",
        "    # example split\n",
        "    remainder = 0\n",
        "    for j in range(len(unique)):\n",
        "        # get all examples with attribute unique[j]\n",
        "        examples_unique = [example for example in examples if example[attr_index] == unique[j]]\n",
        "        remainder += len(examples_unique)/len(examples)*calc_entropy(examples_unique, cls_index)\n",
        "    return calc_entropy(examples, cls_index) - remainder\n",
        "\n",
        "\n",
        "def majority(examples, attr_index):\n",
        "    \"\"\"Returns the value of attribute \"attr_index\" that occurs the most often in the examples.\"\"\"\n",
        "    # create a flat list of all attribute values (with duplicates, so we can count)\n",
        "    attr_vals = [example[attr_index] for example in examples]\n",
        "\n",
        "    # among all unique attribute values, find the maximum with regards to occurrence in the attr_vals list\n",
        "\n",
        "    maj_attr_val = max(set(attr_vals), key=attr_vals.count)\n",
        "    return maj_attr_val\n",
        "\n",
        "def choose_best_attr(examples, attr_avail, cls_index):\n",
        "    \"\"\"Iterates over all available attributes, calculates their information gain and returns the one\n",
        "    that achieves the highest. attr_avail is a list of booleans corresponding to the list of attributes.\n",
        "    it is true if the attribute has not been used in the tree yet (and is not numeric).\n",
        "    \"\"\"\n",
        "    igs = []  # list of information gains for each attribute\n",
        "\n",
        "    for j in range(len(attr)):\n",
        "        if attr_avail[j]:\n",
        "            igs.append(calc_ig(examples, j, cls_index))\n",
        "        else:\n",
        "            igs.append(-1)\n",
        "\n",
        "    return igs.index(max(igs))  # return index of the attribute with highest IG\n",
        "\n",
        "\n",
        "def dtree_learning(examples, attr_avail, default, cls_index):\n",
        "    \"\"\"Implementation of the decition tree learning algorithm according to the pseudo code\n",
        "    in the lecture. Receives the remaining examples, the remaining attributes (as boolean list),\n",
        "    the default label and the index of the class label in the attribute vector.\n",
        "    Returns the root node of the decision tree. \n",
        "\n",
        "    Each tree node is a tuple where the first entry is the index of the \n",
        "    attribute that has been used for the split. It is \"None\" for leaf nodes.\n",
        "    The second entry is a list of subtrees of the same format. The subtrees are ordered in the\n",
        "    same way as the attribute values in \"attr\". For leaf nodes, the second entry is the predicted class.\n",
        "\n",
        "    uses choose_best_attr, majority, dtree_learning\n",
        "    \"\"\"\n",
        "    global attr\n",
        "    if len(examples) == 0:\n",
        "        # is leaf\n",
        "        # no examples left -> majority class prediction of parent\n",
        "        return [None, default]\n",
        "    elif len({x[cls_index] for x in examples}) == 1:\n",
        "        # is leaf\n",
        "        # uniform classes\n",
        "        return [None, examples[0][cls_index]]\n",
        "    elif attr_avail.count(True) == 0:\n",
        "        # is leaf\n",
        "        # no attribute left to split -> majority class prediction\n",
        "        return [None, majority(examples, cls_index)]\n",
        "    else:\n",
        "        best = choose_best_attr(examples, attr_avail, cls_index)\n",
        "        tree = [best, []]\n",
        "        for v in attr[best]:\n",
        "            examples_v = [x for x in examples if x[best] == v]\n",
        "            # why is it important to make a copy and not change it directly?\n",
        "            new_attr_avail = attr_avail.copy()\n",
        "            new_attr_avail[best] = False\n",
        "            subtree = dtree_learning(examples_v, new_attr_avail, majority(examples, cls_index), cls_index)\n",
        "            tree[1].append(subtree)\n",
        "        return tree\n",
        "\n",
        "\n",
        "def dtree_classify(dtree, x):\n",
        "    \"\"\"Classifies a single example x using the given decision tree. Returns the predicted class label.\n",
        "    \"\"\"\n",
        "    # attribute index of splitting attribute\n",
        "    attr_split_index = dtree[0]\n",
        "\n",
        "    if attr_split_index is not None:\n",
        "        # get attribute value for example\n",
        "        example_split_attr = x[attr_split_index]\n",
        "\n",
        "        # subtree position\n",
        "        subtree_pos = attr[attr_split_index].index(example_split_attr)\n",
        "\n",
        "        return dtree_classify(dtree[1][subtree_pos], x)  # descend into subtree recursively\n",
        "    else:\n",
        "        return dtree[1]\n",
        "\n",
        "\n",
        "def dtree_test(dtree, examples, cls_index):\n",
        "    \"\"\"Classify all examples using the given decision tree. Prints the achieved accuracy.\"\"\"\n",
        "    correct = 0\n",
        "    for j in range(len(examples)):\n",
        "        if dtree_classify(dtree, examples[j]) == examples[j][cls_index]:\n",
        "            correct += 1\n",
        "\n",
        "    print(\"{} out of {} correct ({:.2f}%)\".format(correct, len(examples), correct / len(examples) * 100))\n",
        "\n",
        "\n",
        "path = \"./\"  # directory of your data\n",
        "datafile = \"adult.data.txt\"\n",
        "testfile = \"adult.test.txt\"\n",
        "examples, attr = openfile(path, datafile)  # load the training set\n",
        "test, test_attr = openfile(path, testfile)  # load the test set\n",
        "cls_index = len(attr) - 1  # the last attribute is assumed to be the class label\n",
        "# attr_names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"class\"]\n",
        "\n",
        "attr_avail = []  # marks which attributes are available for splitting (not numeric and not the class label)\n",
        "for i in range(len(attr)):\n",
        "    # the list attr[i] contains all possible values of attribute i. It is empty for numeric attributes.\n",
        "    attr_avail.append(len(attr[i]) > 0 and i != cls_index)\n",
        "\n",
        "# print(attr_avail)\n",
        "dtree = dtree_learning(examples, attr_avail, [], cls_index)\n",
        "\n",
        "print(\"Before removal of unknowns: \")\n",
        "print(\"Training Set\")\n",
        "dtree_test(dtree, examples, cls_index)\n",
        "print(\"Test Set\")\n",
        "dtree_test(dtree, test, cls_index)\n",
        "\n",
        "# Extra task removal of unknowns\n",
        "examples_removed = []\n",
        "test_removed = []\n",
        "\n",
        "for example in examples:\n",
        "    # print(example.__contains__(\"?\"))\n",
        "    if not example.__contains__(\"?\"):\n",
        "        examples_removed.append(example)\n",
        "\n",
        "for example in test:\n",
        "    if not example.__contains__(\"?\"):\n",
        "        test_removed.append(example)\n",
        "\n",
        "# print(len(examples_removed))\n",
        "# print(len(test_removed))\n",
        "dtree = dtree_learning(examples_removed, attr_avail, [], cls_index)\n",
        "\n",
        "print(\"After removal of unknowns: \")\n",
        "print(\"Training Set\")\n",
        "dtree_test(dtree, examples_removed, cls_index)\n",
        "print(\"Test Set\")\n",
        "dtree_test(dtree, test_removed, cls_index)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}